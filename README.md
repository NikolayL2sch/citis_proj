# Citis Project
## Обзор

Проект представляет собой инструмент для скрапинга и парсинга данных о продуктах с сайтов электронной коммерции. Используя ***Selenium*** для работы с динамическим веб-контентом и ***BeautifulSoup*** для парсинга **HTML**, проект позволяет пользователям извлекать, фильтровать и сохранять информацию о продуктах в структурированном формате **JSON**.


## Особенности

- Веб-скрапинг: Использует Selenium для навигации и скрапинга динамических веб-страниц.  
- Парсинг HTML: Использует BeautifulSoup для извлечения конкретных данных из HTML.  
- Фильтрация данных: Фильтрует извлеченные данные на основе предопределенных критериев.  
- Вывод JSON: Сохраняет информацию о продуктах в файл JSON.  
- Интеграция с API: Отправляет данные JSON на указанный эндпоинт для дальнейшей обработки.

## Требования

*Python 3.7+  
Selenium  
BeautifulSoup4  
Requests*

## Установка

#### Клонируйте репозиторий:
```bash
git clone https://github.com/NikolayL2sch/citis_proj.git  
cd citis_proj
```
#### Создайте и активируйте виртуальное окружение:
```bash
python -m venv venv  
source venv/bin/activate   # Для Windows используйте `venv\Scripts\activate`
```
#### Установите зависимости:  
```bash
pip install -r requirements.txt
```
#### Скачайте WebDriver
Убедитесь, что у вас установлен правильный *WebDriver* для вашего браузера (например, *ChromeDriver* для *Google Chrome*). Поместите исполняемый файл *WebDriver* в каталог, включенный в **PATH** вашей системы.

**Важно**: в проекте уже есть подвязка к файлу *chromedriver.exe*, который есть в репозитории, однако для корректной работы этого драйвера необходима установленная версия браузера ***Google Chrome <= 114.xx.xx***. Для использования других браузеров, (например, mozilla firefox, вам нужно будет самим установить для них драйвер и прописать его в parser.py:
```python
service = webdriver.chrome.service.Service('./chromedriver.exe')  # Укажите путь к вашему chromedriver
```

**Ссылка на скачивание драйвера для Mozilla Firefox:** https://github.com/mozilla/geckodriver/releases

## Конфигурация  
Отредактируйте файл *config.py*, чтобы настроить необходимые параметры, такие как *URL* целевого сайта и другие параметры скрапинга.
#### В файле config.py: 
- *marketplaces* - список доменных имен маркетплейсов
- *max_websites_count* - переменная, отвечающая за максимальное количество сайтов, которые вы хотите отпарсить, не учитывая отфильтрованные страницы (они не рассматриваются)

> *Примите во внимание*, что *не рекомендуется* выставлять значение этой переменной больше **10**, так как это приведет к разрастанию масштабов поиска и значительному увеличению времени выполнения программы
- *max_results_from_website* - максимальное количество товаров, полученное в итоге с одного сайта
> *Примите во внимание*, что на многих сайтах количество товаров на странице с листингом может достигать **100**. В связи с этим *не рекомендуется* выставлять этот параметр выше **20**, так как это приведет к разрастанию масштабов поиска и значительному увеличению времени выполнения программы
- *headers* - это набор параметров для http-запроса к сайту. Он нужен для того, чтобы запросы выглядели максимально близко к ситуации отправки их из браузера пользователем.

- *websites_css_selectors* - это словарь, в котором находятся селекторы для парсинга сайтов. Со временем мы добавим большее количество сайтов и их селекторов, таким образом обеспечив большее покрытые

## Использование  
  
 #### Запустите парсер:  
 ```bash  
 python parser.py
 ```
#### Сохраните статьи
Используйте функцию *save_articles* для отправки обработанных данных на эндпоинт. Эта функция определена в модуле *api_interaction.py*.

